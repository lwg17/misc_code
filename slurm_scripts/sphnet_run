#!/usr/bin/env bash
# Submit any Python script under the 'sphnet' conda environment via SLURM.
# Install: save as ~/bin/sphnet_run and chmod +x ~/bin/sphnet_run
# Usage:   sphnet_run <script.py> [args...]
# Output:  "Submitted job <jobid>"

set -Eeuo pipefail

# ---------- defaults (override via env) ----------
TIME="${TIME:-72:00:00}"
MEM="${MEM:-128G}"
CPUS="${CPUS:-8}"
GPUS="${NGPUS:-1}"            # GPUs requested
PARTITION="${PARTITION:-}"     # empty => auto-detect GPU partitions; set to 'auto' or a name to control behavior
QOS="${QOS:-}"
MAIL_USER="lwg17@scarletmail.rutgers.edu"
MAIL_TYPE="${MAIL_TYPE:-BEGIN,END,FAIL,REQUEUE}"

# ---------- args ----------
if [ $# -lt 1 ]; then
  echo "Usage: sphnet_run <script.py> [args...]"
  exit 1
fi

CALL_PWD="$(pwd)"
WD="${WD:-$CALL_PWD}"          # working directory inside the job
SCRIPT_ARG="$1"; shift

# Resolve absolute script path based on where you invoked the command
if [[ "$SCRIPT_ARG" = /* ]]; then
  SCRIPT_PATH="$SCRIPT_ARG"
else
  SCRIPT_PATH="$CALL_PWD/$SCRIPT_ARG"
fi
if [ ! -f "$SCRIPT_PATH" ]; then
  echo "[error] Could not find $SCRIPT_ARG (resolved to $SCRIPT_PATH)" >&2
  exit 2
fi

# Logs go under the working dir
LOG_DIR="${LOG_DIR:-$WD/logs}"
[[ "$LOG_DIR" != /* ]] && LOG_DIR="$WD/$LOG_DIR"
mkdir -p "$LOG_DIR"

JOBNAME="sphnetNN"   # always this literal job name

# ---------- sbatch payload ----------
TMP=$(mktemp /tmp/sphnet_${USER}_XXXXXX.sbatch)
{
  echo "#!/usr/bin/env bash"
  echo "#SBATCH --job-name=${JOBNAME}"
  echo "#SBATCH --time=${TIME}"
  echo "#SBATCH --mem=${MEM}"
  echo "#SBATCH --cpus-per-task=${CPUS}"
  echo "#SBATCH --gres=gpu:${GPUS}"
  echo "#SBATCH --output=${LOG_DIR}/${JOBNAME}-%j.out"
  echo "#SBATCH --error=${LOG_DIR}/${JOBNAME}-%j.err"
  echo "#SBATCH --mail-user=${MAIL_USER}"
  echo "#SBATCH --mail-type=${MAIL_TYPE}"
  echo "#SBATCH --requeue"
  echo "#SBATCH --chdir=${WD}"
  [[ -n "${QOS}" ]] && echo "#SBATCH --qos=${QOS}"

  cat <<'EOSB'
# Requeue on preemption (SIGTERM)
requeue_on_term() { scontrol requeue "$SLURM_JOB_ID"; exit 0; }
trap requeue_on_term TERM

# Keep threads modest (avoid oversubscription with dataloaders / BLAS)
export OMP_NUM_THREADS="${OMP_NUM_THREADS:-1}"
export MKL_NUM_THREADS="${MKL_NUM_THREADS:-1}"
export OPENBLAS_NUM_THREADS="${OPENBLAS_NUM_THREADS:-1}"

# Disable W&B by default (user can override by exporting WANDB_DISABLED="")
: "${WANDB_DISABLED:=true}"; export WANDB_DISABLED

# NCCL: use modern async error handling
export TORCH_NCCL_ASYNC_ERROR_HANDLING=1

# Prefer direct Python from the 'sphnet' env (faster than conda activate)
if [ -x "$HOME/.conda/envs/sphnet/bin/python" ]; then
  PY_EXE="$HOME/.conda/envs/sphnet/bin/python"
elif [ -x "$HOME/miniconda3/envs/sphnet/bin/python" ]; then
  PY_EXE="$HOME/miniconda3/envs/sphnet/bin/python"
elif [ -x "$HOME/anaconda3/envs/sphnet/bin/python" ]; then
  PY_EXE="$HOME/anaconda3/envs/sphnet/bin/python"
else
  PY_EXE=""
fi

if [ -z "$PY_EXE" ]; then
  # Fallback: conda shell hook activation
  if [ -f "$HOME/.bashrc" ]; then source "$HOME/.bashrc"; fi
  if command -v conda >/dev/null 2>&1; then eval "$(conda shell.bash hook)"; fi
  conda activate sphnet
  PY_EXE="$(command -v python)"
fi

# Run the target script with Python (never as a shell script)
exec "$PY_EXE" "__SCRIPT_PATH__" "$@"
EOSB
} > "$TMP"

# Substitute the absolute script path safely
sed -i -e "s|__SCRIPT_PATH__|$SCRIPT_PATH|g" "$TMP"

# ---------- submission with GPU partition auto-discovery ----------
try_submit() {
  local part="$1"; shift || true
  if [ -n "$part" ]; then
    sbatch -p "$part" "$TMP" "$@"
  else
    sbatch "$TMP" "$@"
  fi
}

if [ -n "$PARTITION" ] && [ "$PARTITION" != "auto" ]; then
  out="$(try_submit "$PARTITION" "$@" 2>&1)" || { echo "$out" >&2; exit 1; }
  jobid="$(awk '{print $4}' <<<"$out")"
  echo "Submitted job $jobid"

  # --- Hook: log GPU/memory/I/O via dependent Slurm job ---
  if [ -x "$HOME/bin/gpu_job_usage.sh" ]; then
    LOG_PART="${LOG_PART:-main}"
    sbatch --parsable \
      --job-name="gpu-usage-${jobid}" \
      --partition="${LOG_PART}" \
      --time=00:05:00 \
      --cpus-per-task=1 --mem=512M \
      --dependency=afterany:${jobid} \
      --output="$HOME/.gpu_job_usage.${jobid}.out" \
      --wrap "sleep 90; OUTPUT_SIZE_GIB='${OUTPUT_SIZE_GIB:-}' $HOME/bin/gpu_job_usage.sh ${jobid} --wait 10 --retries 60" >/dev/null || true
  fi
  # --- End Hook ---
  exit 0
fi

# Build candidate GPU partitions list via sinfo
declare -a cands=()
if command -v sinfo >/dev/null 2>&1; then
  while IFS= read -r p; do
    # strip trailing '*' and whitespace
    p="${p%%[*]}"; p="${p//\*/}"; p="${p%% }"; p="${p## }"
    # keep unique, gpu-ish
    if [[ "$p" =~ [Gg][Pp][Uu] ]] && [[ ! " ${cands[*]-} " =~ " $p " ]]; then
      cands+=("$p")
    fi
  done < <(sinfo -h -o "%P" 2>/dev/null)
fi

# Always append empty (default partition) as a last resort
cands+=("")

for p in "${cands[@]}"; do
  out="$(try_submit "$p" "$@" 2>&1)" || continue
  jobid="$(awk '{print $4}' <<<"$out")"
  if [ -n "$jobid" ]; then
    echo "Submitted job $jobid"

  # --- Hook: log GPU/memory/I/O via dependent Slurm job ---
  if [ -x "$HOME/bin/gpu_job_usage.sh" ]; then
    LOG_PART="${LOG_PART:-main}"
    sbatch --parsable \
      --job-name="gpu-usage-${jobid}" \
      --partition="${LOG_PART}" \
      --time=00:05:00 \
      --cpus-per-task=1 --mem=512M \
      --dependency=afterany:${jobid} \
      --output="$HOME/.gpu_job_usage.${jobid}.out" \
      --wrap "sleep 90; OUTPUT_SIZE_GIB='${OUTPUT_SIZE_GIB:-}' $HOME/bin/gpu_job_usage.sh ${jobid} --wait 10 --retries 60" >/dev/null || true
  fi
  # --- End Hook ---
    exit 0
  fi
done

# If we got here, all attempts failed
if [ "${#cands[@]}" -gt 0 ]; then
  echo "Error: Unable to submit GPU job. Tried partitions: ${cands[*]} (and default)" >&2
else
  echo "Error: Unable to submit GPU job. Tried default partition only." >&2
fi
exit 1
